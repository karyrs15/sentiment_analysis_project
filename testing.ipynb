{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"testing_file_sentiment_analysis.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dhbgY8sWEi4v"},"source":["## Sentiment Analysis"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gVWI1VtSEnEO","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/My Drive/Colab Notebooks/sentiment analysis project'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"h2869QZMEi7g","colab":{}},"source":["from keras.models import load_model\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import nltk\n","import re\n","import pickle\n","\n","# download stopwords (we're gonna need it later)\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","# clean text to remove users, links and stopwords and then split it in tokens\n","def clean_text(text):\n","    text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n","    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n","    \n","    tokens = text.split()\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [w for w in tokens if w not in stop_words]\n","    \n","    return \" \".join(tokens)\n","\n","# decode score prediction from the model, to be 0 or 1\n","def decode_prediction(prediction):\n","    return 'Negative' if prediction < 0.5 else 'Positive'\n","\n","max_length = 50\n","\n","# load model\n","model = load_model('model_final.h5')\n","# loading tokenizer\n","with open('tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)\n","\n","# test model with a new query\n","tweet = \"im happy!!!\"\n","\n","# clean query text\n","input_text = clean_text(tweet)\n","# tokenize and pad query test as in training\n","input_text = pad_sequences(tokenizer.texts_to_sequences([input_text]),\n","                        maxlen = max_length)\n","\n","# get model prediction\n","prediction = model.predict([input_text])[0]\n","# get decode prediction\n","label = decode_prediction(prediction)\n","\n","print(\"Tweet: \\n\\n{}\\n\".format(tweet))\n","print(\"Score: {} Label: {}\".format(prediction, label))"],"execution_count":0,"outputs":[]}]}