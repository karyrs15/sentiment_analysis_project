{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules needeed\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.layers.core import Dense, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# download stopwords (we're gonna need it later)\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(\"./data/training.1600000.processed.noemoticon.csv\", encoding = 'latin', header = None)\n",
    "# show the first five rows of data (to check if everything's fine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change columns names for reference\n",
    "df.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "# show the first five rows of data (now it has proper headers)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop useless columns\n",
    "df = df.drop(['id', 'date', 'flag', 'user'], axis = 1)\n",
    "# show the first five rows of data (to verify again)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count dataset samples to know how many of each class we have\n",
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text to remove users, links and stopwords and then split it in tokens\n",
    "def clean_text(text):\n",
    "    text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    \n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# apply clean_text function in our data\n",
    "df.text = df.text.apply(lambda x: clean_text(x))\n",
    "# show the first five rows of data (to verify again)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 80% for our data to train\n",
    "train_size = 0.8\n",
    "\n",
    "# split our data into train set (80%) and test set (20%)\n",
    "train_data, test_data = train_test_split(df, test_size = 1 - train_size, random_state = 0, stratify = df.target)\n",
    "\n",
    "# length of each set\n",
    "print(\"Train data size: \", len(train_data))\n",
    "print(\"Test data size: \", len(test_data))\n",
    "\n",
    "# how many examples of each class there is in each set\n",
    "print(\"Train data distr: \\n\", train_data.target.value_counts())\n",
    "print(\"Test data distr: \\n\", test_data.target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer in the train text\n",
    "tokenizer.fit_on_texts(train_data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max length of the train data\n",
    "max_length = max([len(s.split()) for s in train_data.text])\n",
    "\n",
    "# pad sequences in x_train data set to the max length\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text),\n",
    "                        maxlen = max_length)\n",
    "# pad sequences in x_test data set to the max length\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text),\n",
    "                       maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a label encoder\n",
    "encoder = LabelEncoder()\n",
    "# enconde labels (0 or 1) in train data\n",
    "encoder.fit(train_data.target.to_list())\n",
    "\n",
    "# transform labels in y_train and y_test data to the encoded ones\n",
    "y_train = encoder.transform(train_data.target.to_list())\n",
    "y_test = encoder.transform(test_data.target.to_list())\n",
    "\n",
    "# reshape y_train and y_test data\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r',encoding=\"utf-8\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    "\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, embedding_dim))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "    return weight_matrix\n",
    "\n",
    "# contains the index for each word\n",
    "vocab = tokenizer.word_index\n",
    "# total number of words in our vocabulary, plus one for unknown words\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# embedding dimensions\n",
    "embedding_dim = 300\n",
    "\n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('./data/glove/glove.6B.300d.txt')\n",
    "# get vectors in the right order\n",
    "embedding_matrix = get_weight_matrix(raw_embedding, vocab)\n",
    "\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, \n",
    "                            embedding_dim, \n",
    "                            weights = [embedding_matrix], \n",
    "                            input_length = max_length, \n",
    "                            trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100, dropout = 0.2))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10\n",
    "\n",
    "ReduceLROnPlateau = ReduceLROnPlateau(factor = 0.1,\n",
    "                                     min_lr = 0.01,\n",
    "                                     monitor = 'val_loss',\n",
    "                                     verbose = 1)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs = EPOCHS,\n",
    "                    validation_split = 0.1, callbacks = [ReduceLROnPlateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('third_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(x_test, y_test, batch_size = BATCH_SIZE)\n",
    "print(\"Test accuracy:\", score[1])\n",
    "print(\"Test loss:\", score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting model performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode score prediction from the model, to be 0 or 1\n",
    "def decode_prediction(prediction):\n",
    "    return 0 if prediction < 0.5 else 1\n",
    "\n",
    "# load model\n",
    "model = load_model('third_model.h5')\n",
    "\n",
    "# test model with a new query\n",
    "data_string = clean_text(\"@taylorswift i love it! it's amazing and im scared\")\n",
    "# tokenize and pad query test as in training\n",
    "data_string = pad_sequences(tokenizer.texts_to_sequences([data_string]),\n",
    "                        maxlen = max_length)\n",
    "\n",
    "# get model prediction\n",
    "prediction = model.predict([data_string])[0]\n",
    "# get decode prediction\n",
    "label = decode_prediction(prediction)\n",
    "\n",
    "print(\"Prediction: {} Score: {}\".format(prediction, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
